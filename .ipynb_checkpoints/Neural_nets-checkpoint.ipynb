{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45733405-65a4-4c31-937a-019e26566751",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Neural Network Implementation</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c617de4c-480c-4a3e-b366-9322bb86749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cb3f47f-e8f5-4735-acaa-0d3b12e48d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self,n_neurons,n_inputs):\n",
    "        self.w=np.random.randn(n_neurons,n_inputs).T #already storing W as W.T\n",
    "        self.b=np.zeros(1,n_neurons)\n",
    "    def forward(self,X): #assuming X has dimns m*n_features\n",
    "        self.X=X\n",
    "        self.output=np.dot(X,self.w) + self.b\n",
    "    def backward(self,dvalues):\n",
    "        self.dw=np.dot(self.X.T,dvalues)\n",
    "        self.db=np.sum(dvalues*1,axis=0,keepdims=True)\n",
    "        self.dinputs=np.dot(dvalues,self.w.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071f7c5-8d85-4894-8212-262906995e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf6fd55f-2463-46d7-bd31-f7edb8e871b5",
   "metadata": {},
   "source": [
    "## ACTIVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d394178c-ce58-4369-aa17-8104e604fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLu:\n",
    "    def forward(self,inp):\n",
    "        self.output=np.maximum(0,inp)\n",
    "        self.inputs=inp.copy()\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2a6642-2732-4807-b013-a00a260dbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self,inp):\n",
    "        self.output=1./(1+np.exp(-inp))\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs=(dvalues*self.output(1-self.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ccba369-8d5f-4030-bc6e-d7ea634cc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self,inp):# overflow prevention present\n",
    "        self.output=(np.exp(inp)- np.max(inp, axis=1, keepdims=True))/(np.sum(np.exp(inp),axis=1))\n",
    "    def backward(self,dvalues):\n",
    "        for i,output,dvalue in enumerate(zip(self.output,dvalues)):\n",
    "            dinputs[i]=np.dot((np.diagflat(output)-np.dot(output,output.T)),dvalue)\n",
    "#dot prod of jacobian of a single output and its derivative values row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430cdf17-ffc4-4063-8bdc-2cd7eeafc68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLu:\n",
    "    def forward(self,inp):\n",
    "         self.output=np.maximum(0.01*inp,inp)\n",
    "         self.inputs=inp.copy()\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cfa518a-447a-432d-ae10-e64506b832ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh:\n",
    "    def forward(self,inp):\n",
    "        self.output=(exp(2*inp)-1)/(exp(2*x)+1)\n",
    "        self.inputs=inp.copy()\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs=1-np.tan(dvalues)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490b6808-e300-4cc1-9680-b80c5d1a7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Par_ReLu:\n",
    "    def __init__(self,alpha=0.25):\n",
    "        self.a=alpha\n",
    "    def forward(self,inp):\n",
    "        self.inputs=inp.copy()\n",
    "        self.output=np.where[self.inputs>0,self.inputs,self.a*self.inputs]\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs=dvalues*np.where[self.inputs>0,1,self.a]\n",
    "        self.dalpha = np.sum(dvalues[self.inputs <= 0] * self.inputs[self.inputs <= 0])\n",
    "    def update(self,learning_rate=0.1):\n",
    "        self.a=self.a-learning_rate*self.dalpha\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3172eeee-1773-418d-87bc-5a6b2f9dadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELu:\n",
    "    def forward(self,inp,alpha=1.6732):\n",
    "        self.input=inp.copy()\n",
    "        self.output=np.where(input>0,input,alpha*(exp(x)-1))\n",
    "    def backward(self,dvalues,):\n",
    "        self.dinput=dvalues*np.where(input>0,1,self.alpha*exp(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19178d5-2943-4504-b2d9-46b58fa8a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeLu:\n",
    "    def forwaard(self,inp,L=1.0507,alpha=1.6732):\n",
    "        self.inputs=inp.copy()\n",
    "        self.output=l*np.where(inputs>0,input,alpha*(exp(input)-1))\n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs=L*dvalues*np.where(inputs>0,1,alpha*exp(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0830e-8f2b-487b-aee3-35349eab0917",
   "metadata": {},
   "source": [
    "## LOSS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31ff14f6-da50-4fd6-b07e-902ebb9a4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyLoss:\n",
    "    def calculate(self,y_hat,y_pred):# y-hat and y_pred are matrixes of dimn(m,1)\n",
    "        self.loss=(1./y_hat.shape[0])*np.dot(y_hat.reshape(1,-1),y_pred)\n",
    "   # def regularization():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4eee684d-b34c-467b-8ae3-5ff36f7607fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatergoricalCrossEntropyLoss:\n",
    "    def calculate(self,y_act,y_pred):# assuming y_act(the labels-actual values) are one hot encoded OR as a m*1 vector consisting of correct category as 1,2,..no of categories\n",
    "        if (y_act.ndim == 1) or (y_act.ndim == 2 and y_act.shape[1] == 1):\n",
    "              y_act=np.eyes(y_pred.shape[1])[y_act.flatten-1] #.flatten to ensure its 1D, by GPT\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15) # to avoid log(0), by sentnex\n",
    "        self.cost=-np.sum(y_act*np.log(y_pred),axis=1)\n",
    "    #def regularization function to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f78236f-87e2-4584-b83e-6afd11642f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    def calculate(self,y_act,y_pred):\n",
    "        self.cost=(y_pred-y_act)**2).mean()/2#axis not determined as both will be 1D matrices\n",
    "        #self.cost = (0.5 / y_pred.shape[0]) * np.dot((y_act - y_pred).T, (y_act - y_pred))\n",
    "        #CS229 FORMULA commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eecd7e-8aee-402f-8dee-1e7ef20679cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE:\n",
    "    def calculate(self,y_act,y_pred):\n",
    "        self.cost=np.abs(y_act-y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc94a5d4-382e-4c4f-90ff-b3f0d0aaae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss:\n",
    "    def calculate(self,y_act,y_pred,delta=1.0):\n",
    "        diffn=y_act-y_pred\n",
    "        self.cost=np.where(np.abs(diffn)<delta,(.5)*diffn**2,delta*np.abs(diffn)-(0.5)*delta**2)#sexy boi\n",
    "        self.cost=np.mean(self.cost) #np.mean() and value.mean() both are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e4e7b-895e-4def-bcc9-eccefae0dcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91ae22e8-f6c1-46d8-870f-6f061ffbce2f",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a97bf2d-85e4-43b5-ac0f-9c5c1fa0b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGD:# try to learn how to add mini and stochastic gradient descent in it and also how to shuffle to remove bias \n",
    "    def __init__(self,decay=0,bsize=0,learning_rate=1,mmentum=0):\n",
    "        self.lr=learning_rate\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.momentum=mmentum\n",
    "        self.iterations=0\n",
    "    def pre_update(self,layer):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.lr * (1. / (1. + self.decay * self.iterations))\n",
    "    def update(self,layer):\n",
    "        if self.momentum: \n",
    "            if not hasattr(layer,'weight_momentums'):\n",
    "                  layer.weight_momentums = np.zeros_like(layer.w)\n",
    "                  layer.bias_momentums = np.zeros_like(layer.b)\n",
    "            self.w_updates=self.momentum*self.w_updates+self.current_learning_rate*layer.dw\n",
    "            self.b_updates=self.momentum*self.bias_updates+self.current_learning_rate*layer.db\n",
    "        else:\n",
    "             self.w_updates=self.current_learning_rate*layer.dw\n",
    "             self.b_updates=self.current_learning_rate*layer.db\n",
    "        layer.w -=self.w_updates\n",
    "        layer.b -=self.b_updates\n",
    "    def post_update(self):\n",
    "        self.iterations +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85af5620-82c2-49c9-b69b-9a5bd2637d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAG:\n",
    "    def __init__(self, decay=0, bsize=0, learning_rate=1.0, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "        self.batch_size = bsize\n",
    "\n",
    "    def pre_update(self, layer):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n",
    "\n",
    "        if not hasattr(layer, 'vt'):\n",
    "            layer.vt = np.zeros_like(layer.w)\n",
    "            layer.bias_momentums = np.zeros_like(layer.b)\n",
    "\n",
    "        layer.w -= self.momentum * layer.vt\n",
    "        layer.b -= self.momentum * layer.bias_momentums\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.vt = self.momentum * layer.vt + self.current_learning_rate * layer.dw\n",
    "        layer.bias_momentums = self.momentum * layer.bias_momentums + self.current_learning_rate * layer.db\n",
    "\n",
    "        layer.w -= layer.vt\n",
    "        layer.b -= layer.bias_momentums\n",
    "\n",
    "    def post_update(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d51c3578-e049-4a37-8b14-e137cdf3d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self,learning_rate,epilson=1e-8):\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.iterations=0\n",
    "        self.epilson=epilson\n",
    "    def pre_update(self,layer,):\n",
    "        if not hasattr(layer,'vt'):\n",
    "            layer.vt = np.zeros_like(layer.w)\n",
    "            layer.bias_vt = np.zeros_like(layer.b)\n",
    "        layer.vt=layer.vt+ (layer.dw)**2\n",
    "        layer.bias_vt=layer.bias_vt+ (layer.db)**2\n",
    "        \n",
    "                                                                     \n",
    "    def update(self,layer):\n",
    "        w_updates = self.current_learning_rate * layer.dw / (np.sqrt(layer.vt) + self.epilson)\n",
    "        b_updates = self.current_learning_rate * layer.db / (np.sqrt(layer.bias_vt) + self.epilson)\n",
    "        layer.w -=w_updates\n",
    "        layer.b -=b_updates\n",
    "        \n",
    "    def post_update(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "612ddfbd-5646-44b9-9261-0428605ce1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    def __init__(self,beta,learning_rate,epilson=1e-8):\n",
    "        self.lr=learning_rate\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.iterations=0\n",
    "        self.beta=beta\n",
    "        self.epilson=epilson\n",
    "        self.iterations=0\n",
    "    def pre_update(self,layer,):\n",
    "        if not hasattr(layer,'vt'):\n",
    "            layer.vt = np.zeros_like(layer.w)\n",
    "            layer.bias_vt = np.zeros_like(layer.b)\n",
    "        layer.vt=self.beta*layer.vt+ (1-self.beta)*(layer.dw)**2\n",
    "        layer.bias_vt=self.beta*layer.bias_vt+ (1-self.beta)*(layer.db)**2\n",
    "    def update(self,layer):\n",
    "        w_updates = self.current_learning_rate * layer.dw / (np.sqrt(layer.vt) + self.epsilon)\n",
    "        b_updates = self.current_learning_rate * layer.db / (np.sqrt(layer.bias_vt) + self.epsilon)\n",
    "        layer.w -=w_updates\n",
    "        layer.b -=b_updates\n",
    "        \n",
    "    def post_update(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b331ff-8d70-41cb-9634-ae68ea7b0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self,learning_rate,decay=0,beta1=0.9,beta2=0.99,epilson=1e-8):\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.lr=learning_rate\n",
    "        self.current_learning_rate=learning_rate\n",
    "        self.iterations=1\n",
    "        self.epilson=epilson\n",
    "        self.decay=decay\n",
    "    def pre_update(self,layer):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.lr * (1. / (1. + self.decay * self.iterations))\n",
    "        if not hasattr(layer,'vt'):\n",
    "            layer.mt = np.zeros_like(layer.w)\n",
    "            layer.bias_mt = np.zeros_like(layer.b)\n",
    "            \n",
    "            layer.vt = np.zeros_like(layer.w)\n",
    "            layer.bias_vt = np.zeros_like(layer.b)\n",
    "\n",
    "        layer.mt=self.beta1*layer.mt+(1-self.beta1)*layer.dw\n",
    "        layer.bias_mt=self.beta1*layer.bias_mt+(1-self.beta1)*layer.db\n",
    "        \n",
    "        layer.vt=self.beta2*layer.vt+(1-self.beta2)*(layer.dw)**2\n",
    "        layer.bias_vt=self.beta2*layer.bias_vt+(1-self.beta2)*(layer.db)**2\n",
    "\n",
    "        layer.mbias=layer.mt/(1-self.beta1**self.iterations)\n",
    "        layer.vbias=layer.vt/(1-self.beta2**self.iterations)\n",
    "        \n",
    "        layer.mbbias=layer.bias_mt/(1-self.beta1**self.iterations)\n",
    "        layer.vbbias=layer.bias_vt/(1-self.beta2**self.iterations)\n",
    "    def update(self,layer):\n",
    "        w_updates = self.current_learning_rate * layer.mbias / (np.sqrt(layer.vbias) + self.epilson)\n",
    "        b_updates = self.current_learning_rate * layer.mbbias/ (np.sqrt(layer.vbbias) + self.epilson)\n",
    "        layer.w -=w_updates\n",
    "        layer.b -=b_updates\n",
    "    def post_update(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607ade4-d2ed-4b02-8923-39dfebb847f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
